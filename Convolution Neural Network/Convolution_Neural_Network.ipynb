{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolution Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 합성곱 및 풀링계층 구현"
      ],
      "metadata": {
        "id": "XlKPUNb52ua3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M1Ok9kQU2hoo"
      },
      "outputs": [],
      "source": [
        "# im2col -> image to column\n",
        "\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "  # input_data: 4차원 배열 형태의 입력데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "  # filter_h : 필터의 높이\n",
        "  # filter_w : 필터의 너비\n",
        "  # returns: col(2차원 배열)\n",
        "\n",
        "  N, C, H, W = input_data.shape\n",
        "  out_h = (H+ 2*pad - filter_h) // stride + 1\n",
        "  out_w = (W+ 2*pad - filter_w) // stride + 1\n",
        "\n",
        "  img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad,pad)], 'constant')\n",
        "  col = np.zeros((N,C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "  for y in range(filter_h):\n",
        "    y_max = y + stride*out_h\n",
        "    for x in range(filter_w):\n",
        "      x_max = x + stride*out_w\n",
        "      col[:,:,y,x,:,:] = img[:,:,y:y_max:stride, x:x_max:stride]\n",
        "  col = col.transpose(0,4,5,1,2,3).reshape(N*out_h*out_w, -1)\n",
        "  return col\n",
        "\n",
        "\n",
        "  # col2im -> column to image\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    # col: 2차원 배열(입력데이터)\n",
        "    # input_shape : 원래 이미지 데이터의 형상\n",
        "    # return : img: 변환된 이미지\n",
        "\n",
        "  N, C, H, W = input_shape\n",
        "  out_h = (H + 2*pad - filter_h) // stride + 1\n",
        "  out_w = (W + 2*pad - filter_w) // stride + 1\n",
        "  col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0,3,4,5,1,2)\n",
        "\n",
        "  img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "  for y in range(filter_h):\n",
        "    y_max = y + stride * out_h\n",
        "    for x in range(filter_w):\n",
        "      x_max = x + stride*out_w\n",
        "      img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "  return img[:, :, pad:H+pad, pad:W + pad]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 합성곱 구현"
      ],
      "metadata": {
        "id": "ur6sZWg0w1pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolution:\n",
        "  def __init__(self, W, b, stride=1, pad=0):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "    self.x = None\n",
        "    self.col = None\n",
        "    self.col_W = None\n",
        "\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    FN, C, FH, FW = self.W.shape # 필터개수, 채널, 필터높이, 필터너비\n",
        "    N,C,H,W = x.shape\n",
        "    out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
        "    out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "    col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "    col_W = self.W.reshape(FN, -1).T # 2차원 배열로 전개\n",
        "    \n",
        "    out = np.dot(col, col_W) + self.b\n",
        "    out = out.reshape(N, out_h, out_w, -1).transpose(0,3,1,2) # transpose : 다차원 배열의 축 순서를 바꿔 줌\n",
        "    \n",
        "    self.x = x\n",
        "    self.col = col\n",
        "    self.col_W = col_W\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    FN, C, FH, FW = self.W.shape\n",
        "    dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "    self.db = np.sum(dout, axis=0)\n",
        "    self.dW = np.dot(self.col.T, dout)\n",
        "    self.dW = self.dW.transpose(1,0).reshape(FN,C,FH,FW)\n",
        "\n",
        "    dcol = np.dot(dout, self.col_W.T)\n",
        "    dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "    return dx"
      ],
      "metadata": {
        "id": "lzpWBlRZw31r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 풀링 계층"
      ],
      "metadata": {
        "id": "TTLOO_kzg0TD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 입력 데이터를 전개한다.\n",
        "2. 행별 최댓값을 구한다.\n",
        "3. 적절한 모양으로 reshape을 해준다."
      ],
      "metadata": {
        "id": "qJudQGN-jF-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pooling:\n",
        "  def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "    self.pool_h = pool_h\n",
        "    self.pool_w = pool_w\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "    self.x = None\n",
        "    self.arg_max = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "    out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "    col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "    col = col.reshape(-1, self.pool_h * self.pool_w)\n",
        "\n",
        "    arg_max = np.argmax(col, axis=1)\n",
        "    out = np.max(col, axis=1)\n",
        "    out = out.reshape(N, out_h, out_w, C).transpose(0,3,1,2)\n",
        "    \n",
        "    self.x = x\n",
        "    self.arg_max = arg_max\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "    pool_size = self.pool_h * self.pool_w\n",
        "    dmax = np.zeros((dout.size, pool_size))\n",
        "    dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "    dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "    dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "    dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "    return dx"
      ],
      "metadata": {
        "id": "Q4HCni1bg3OS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. CNN 구현\n",
        "  -  CNN 구성: Conv -> ReLU -> Pooling -> ... -> Affine -> ReLU -> Affine -> Softmax"
      ],
      "metadata": {
        "id": "wQqPnDf6VZBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x) # 오버플로 대책\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"
      ],
      "metadata": {
        "id": "YW2nRXPl_ijE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "      self.mask = None\n",
        "    def forward(self, x):\n",
        "      self.mask = (x <= 0)\n",
        "      out = x.copy()\n",
        "      out[self.mask] = 0\n",
        "      return out\n",
        "    def backward(self, out):\n",
        "      out[self.mask] = 0\n",
        "      x = out\n",
        "      return x\n",
        "\n",
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.x = None\n",
        "    self.original_x_shape = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "  \n",
        "  def forward(self, x):\n",
        "    self.original_x_shape = x.shape\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    self.x = x\n",
        "    out = np.dot(self.x, self.W) + self.b\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = np.dot(dout, self.W.T)\n",
        "    self.dW = np.dot(self.x.T, dout)\n",
        "    self.db = np.sum(dout, axis=0)\n",
        "    dx = dx.reshape(*self.original_x_shape)\n",
        "    return dx\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.loss = None\n",
        "    self.y = None\n",
        "    self.t = None\n",
        "\n",
        "  def forward(self,x,t):\n",
        "    self.t = t\n",
        "    self.y = softmax(x)\n",
        "    self.loss = cross_entropy_error(self.y, self.t)\n",
        "    return self.loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    if self.t.size == self.y.size: # in case of one-hot vectoring\n",
        "      dx = (self.y - self.t) / batch_size\n",
        "    else:\n",
        "      dx = self.y.copy()\n",
        "      dx[np.arange(batch_size), self.t] -= 1\n",
        "      dx = dx / batch_size\n",
        "    return dx"
      ],
      "metadata": {
        "id": "cbFC4Hg03Tjr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvNet:\n",
        "  def __init__(self, input_dim = (1,28,28),\n",
        "               conv_param = {\"filter_num\":30, \"filter_size\":5, \"pad\":0, \"stride\":1},\n",
        "               hidden_size = 100, output_size=10, weight_init_std=0.01):\n",
        "    # conv_param: 합성곱 계층의 하이퍼파라미터를 딕셔너리 형태로 주어짐\n",
        "    filter_num = conv_param[\"filter_num\"]\n",
        "    filter_size = conv_param[\"filter_size\"]\n",
        "    filter_pad = conv_param[\"pad\"]\n",
        "    filter_stride = conv_param[\"stride\"]\n",
        "    input_size = input_dim[1]\n",
        "    conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "    pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "    self.params = {}\n",
        "    self.params[\"W1\"] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "    self.params[\"b1\"] = np.zeros(filter_num)\n",
        "    self.params[\"W2\"] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
        "    self.params[\"b2\"] = np.zeros(hidden_size)\n",
        "    self.params[\"W3\"] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params[\"b3\"] = np.zeros(output_size)\n",
        "\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers[\"Conv1\"] = Convolution(self.params[\"W1\"], self.params[\"b1\"], conv_param[\"stride\"], conv_param[\"pad\"])\n",
        "    self.layers[\"Relu1\"] = Relu()\n",
        "    self.layers[\"Pool1\"] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "    self.layers[\"Affine1\"] = Affine(self.params[\"W2\"], self.params[\"b2\"])\n",
        "    self.layers[\"Relu2\"] = Relu()\n",
        "    self.layers[\"Affine2\"] = Affine(self.params[\"W3\"], self.params[\"b3\"])\n",
        "    self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "  def predict(self,x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "    return x\n",
        "  \n",
        "  def loss(self,x,t):\n",
        "    y = self.predict(x)\n",
        "    return self.last_layer.forward(y,t)\n",
        "\n",
        "  def accuracy(self, x, t, batch_size=100):\n",
        "    if t.ndim != 1:\n",
        "      t = np.argmax(t, axis=1)\n",
        "    \n",
        "    acc = 0.0\n",
        "    for i in range(int(x.shape[0] / batch_size)):\n",
        "      tx = x[i*batch_size : (i+1)*batch_size]\n",
        "      tt = t[i*batch_size : (i+1)*batch_size]\n",
        "      y = self.predict(tx)\n",
        "      y = np.argmax(y, axis=1)\n",
        "      acc += np.sum(y == tt)\n",
        "\n",
        "    return acc / x.shape[0]\n",
        "\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    # 순전파\n",
        "    self.loss(x,t)\n",
        "\n",
        "    # 역전파\n",
        "    dout = 1\n",
        "    dout = self.last_layer.backward(dout)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    grads = {}\n",
        "    grads[\"W1\"] = self.layers[\"Conv1\"].dW\n",
        "    grads[\"b1\"] = self.layers[\"Conv1\"].db\n",
        "    grads[\"W2\"] = self.layers[\"Affine1\"].dW\n",
        "    grads[\"b2\"] = self.layers[\"Affine1\"].db\n",
        "    grads[\"W3\"] = self.layers[\"Affine2\"].dW\n",
        "    grads[\"b3\"] = self.layers[\"Affine2\"].db\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "Usasb8ruVsb7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN with MNIST"
      ],
      "metadata": {
        "id": "e-5rixQwKDWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Dataset"
      ],
      "metadata": {
        "id": "nIDQm-7WzW0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from torchvision import datasets\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7j1TQdQ0KHBk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mnist(normalize=True, flatten=True):\n",
        "  # MNIST dataset\n",
        "  mnist_train = datasets.MNIST(root=\"./data/\", train=True, download=True)\n",
        "  mnist_test = datasets.MNIST(root=\"./data/\", train=False, download=True)\n",
        "  print (\"mnist_train:\\n\",mnist_train,\"\\n\")\n",
        "  print (\"mnist_test:\\n\",mnist_test,\"\\n\")\n",
        "  print (\"Done.\")\n",
        "  \n",
        "  train_img = []\n",
        "  train_label = []\n",
        "  for data in mnist_train:\n",
        "    train_img.append(np.array(data[0]).reshape(-1, 784))\n",
        "    train_label.append(data[1])\n",
        "\n",
        "  test_img = []\n",
        "  test_label = []\n",
        "  for test_data in mnist_test:\n",
        "    test_img.append(np.array(test_data[0]).reshape(-1, 784))\n",
        "    test_label.append(test_data[1])\n",
        "  \n",
        "  train_img = np.array(train_img)\n",
        "  train_label = np.array(train_label)\n",
        "  test_img = np.array(test_img)\n",
        "  test_label = np.array(test_label)\n",
        "\n",
        "\n",
        "  if normalize:\n",
        "    train_img = train_img.astype(np.float32)\n",
        "    train_img /= 255.0\n",
        "    test_img = test_img.astype(np.float32)\n",
        "    test_img /= 255.0\n",
        "\n",
        "  if not flatten:\n",
        "    train_img = train_img.reshape(-1, 1, 28, 28)\n",
        "    test_img = test_img.reshape(-1, 1, 28, 28)\n",
        "\n",
        "  return train_img, train_label, test_img, test_label\n",
        "    "
      ],
      "metadata": {
        "id": "rr77bK-bJ32o"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Trainer"
      ],
      "metadata": {
        "id": "H0svv2nWzZxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer\n",
        "class SGD:\n",
        "  def __init__(self, lr=0.01):\n",
        "    self.lr = lr\n",
        "  \n",
        "  def update(self, params, grads):\n",
        "    for key in params.keys():\n",
        "      params[key] -= self.lr * grads[key]\n",
        "\n",
        "class AdaGrad:\n",
        "  def __init__(self, lr=0.01):\n",
        "    self.lr =lr\n",
        "    self.h = None\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    if self.h == None:\n",
        "      self.h = {}\n",
        "      for key, val in params.items():\n",
        "        self.h[key] = np.zeros_like(val)\n",
        "\n",
        "    for key in params.keys():\n",
        "      self.h[key] += grads[key] * grads[key]\n",
        "      params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
      ],
      "metadata": {
        "id": "jJ9Hy2WKjckO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "               epochs=20, mini_batch_size=100,\n",
        "               optimizer=\"SGD\", optimizer_param={\"lr\":0.01},\n",
        "               eval_sample_num_per_epoch=None, verbose=True):\n",
        "    self.network = network\n",
        "    self.verbose = verbose\n",
        "    self.x_train = x_train\n",
        "    self.t_train = t_train\n",
        "    self.x_test = x_test\n",
        "    self.t_test = t_test\n",
        "    self.epochs = epochs\n",
        "    self.mini_batch_size = mini_batch_size\n",
        "    self.eval_sample_num_per_epoch = eval_sample_num_per_epoch\n",
        "    \n",
        "    optimizer_dict = {\"sgd\" : SGD, \"adagrad\" : AdaGrad}\n",
        "    self.optimizer = optimizer_dict[optimizer.lower()](**optimizer_param)\n",
        "    self.train_size = x_train.shape[0]\n",
        "    self.iter_per_epoch = max(self.train_size/mini_batch_size, 1)\n",
        "    self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "    self.current_iter = 0\n",
        "    self.current_epoch = 0\n",
        "    self.train_loss_list = []\n",
        "    self.train_acc_list = []\n",
        "    self.test_acc_list= []\n",
        "\n",
        "  def train_step(self):\n",
        "    batch_mask = np.random.choice(self.train_size, self.mini_batch_size)\n",
        "    x_batch = self.x_train[batch_mask]\n",
        "    t_batch = self.t_train[batch_mask]\n",
        "\n",
        "    grads = self.network.gradient(x_batch, t_batch)\n",
        "    self.optimizer.update(self.network.params, grads)\n",
        "\n",
        "    loss = self.network.loss(x_batch, t_batch)\n",
        "    self.train_loss_list.append(loss)\n",
        "    if self.verbose:\n",
        "      print(\"Train loss: \" + str(loss))\n",
        "\n",
        "    if self.current_iter % self.iter_per_epoch == 0:\n",
        "      self.current_epoch += 1\n",
        "\n",
        "      x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "      x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "      if not self.eval_sample_num_per_epoch is None:\n",
        "        t = self.eval_sample_num_per_epoch\n",
        "        x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "        x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "\n",
        "      train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "      test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "      self.train_acc_list.append(train_acc)\n",
        "      self.test_acc_list.append(test_acc)\n",
        "\n",
        "      if self.verbose:\n",
        "        print(\"=== Epoch: \" + str(self.current_epoch) + \" , Train acc\" + str(train_acc) +  \", Test acc:\" + str(test_acc) + \"===\")\n",
        "      \n",
        "      self.current_iter += 1\n",
        "\n",
        "  def train(self):\n",
        "    for i in range(self.max_iter):\n",
        "      self.train_step()\n",
        "\n",
        "    test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "    if self.verbose:\n",
        "      print(\"=== Final Accuracy: \" + str(test_acc))"
      ],
      "metadata": {
        "id": "1ophYEP-1wI1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, t_train, x_test, t_test = load_mnist(flatten=False)\n",
        "epochs = 1\n",
        "\n",
        "model = SimpleConvNet(input_dim = (1,28,28),\n",
        "                      conv_param = {'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                      hidden_size=100,\n",
        "                      output_size=10,\n",
        "                      weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(model, x_train, t_train, x_test, t_test,\n",
        "                  epochs=epochs, mini_batch_size=1000,\n",
        "                  optimizer=\"AdaGrad\", optimizer_param={\"lr\":0.01},\n",
        "                  eval_sample_num_per_epoch=1\n",
        "                  )\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgtOdyMQlapZ",
        "outputId": "689fbe9d-6b43-42bd-cc08-06fad5d9bd3e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mnist_train:\n",
            " Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./data/\n",
            "    Split: Train \n",
            "\n",
            "mnist_test:\n",
            " Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./data/\n",
            "    Split: Test \n",
            "\n",
            "Done.\n",
            "Train loss: 2.2523328829838816\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "=== Epoch: 1 , Train acc0.0, Test acc:0.0===\n",
            "Train loss: 1.9832398475036326\n",
            "Train loss: 2.383193421870098\n",
            "Train loss: 2.0322255260408078\n",
            "Train loss: 1.781890731400003\n",
            "Train loss: 1.4360545692431752\n",
            "Train loss: 1.2217505867942067\n",
            "Train loss: 1.2637488111417514\n",
            "Train loss: 1.3811452861165425\n",
            "Train loss: 0.9763448875858239\n",
            "Train loss: 0.8712806041031418\n",
            "Train loss: 0.8826276077698287\n",
            "Train loss: 0.9210920902870321\n",
            "Train loss: 0.7339169678410228\n",
            "Train loss: 0.6688200562536263\n",
            "Train loss: 0.6738810589783388\n",
            "Train loss: 0.6510279715674034\n",
            "Train loss: 0.6923606761274116\n",
            "Train loss: 0.5868992202389017\n",
            "Train loss: 0.533599334593654\n",
            "Train loss: 0.4834759372562925\n",
            "Train loss: 0.4788219985634622\n",
            "Train loss: 0.5151125504660294\n",
            "Train loss: 0.46547737562292546\n",
            "Train loss: 0.44232037629431514\n",
            "Train loss: 0.4829607482453505\n",
            "Train loss: 0.4912254633715386\n",
            "Train loss: 0.46607830353954166\n",
            "Train loss: 0.44127087765726997\n",
            "Train loss: 0.42059666952236047\n",
            "Train loss: 0.4575760425529705\n",
            "Train loss: 0.3897441105810358\n",
            "Train loss: 0.38365701706307787\n",
            "Train loss: 0.41428434652194485\n",
            "Train loss: 0.3576466138353935\n",
            "Train loss: 0.33494906427580745\n",
            "Train loss: 0.35443174471600564\n",
            "Train loss: 0.35825401608695334\n",
            "Train loss: 0.3649202723263053\n",
            "Train loss: 0.377613475759023\n",
            "Train loss: 0.43640938424719034\n",
            "Train loss: 0.43419622496162297\n",
            "Train loss: 0.4590607870683495\n",
            "Train loss: 0.32031866088933525\n",
            "Train loss: 0.3823619270383223\n",
            "Train loss: 0.3339225593058354\n",
            "Train loss: 0.35212134455727456\n",
            "Train loss: 0.2974942048143855\n",
            "Train loss: 0.3018035496683473\n",
            "Train loss: 0.329462273718065\n",
            "Train loss: 0.28265930845442405\n",
            "Train loss: 0.2651907489267859\n",
            "Train loss: 0.26943458767620254\n",
            "Train loss: 0.28907258107868933\n",
            "Train loss: 0.3322763524605458\n",
            "Train loss: 0.28312209184358317\n",
            "Train loss: 0.3336790290847341\n",
            "Train loss: 0.3216398307641786\n",
            "Train loss: 0.27279824785072637\n",
            "Train loss: 0.2641327990434881\n",
            "<class 'numpy.ndarray'>\n",
            "=== Final Accuracy: 0.9242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0y2FAqCJR6GK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}